{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cbc8821d-59fe-4be3-93aa-efb836d6c46e",
   "metadata": {},
   "source": [
    "# Dimension Reduction \n",
    "Working directly with high-dimensional data comes with some difficulties: \n",
    "- hard to analyze\n",
    "- interpretation is difficult\n",
    "- visualization is nearly impossible\n",
    "- storage of the data vectors can be expensive.\n",
    "However, dimension reduction can exploit the redundancy and the correlation among dimensions, ideally **accurately** and **without losing information**. We can think of dimension reduction as a **compression** technique. \n",
    "\n",
    "# Problem Formulation\n",
    "We consider an i.i.d. data set $\\mathbf X = \\{\\mathbf x_1, ..., \\mathbf x_N\\}, \\mathbf x_n \\in \\mathbb R^D$. We want to project them into $N$ $M$-dimension vectors where $M << D$. Principal component analysis (PCA) is a **linear** algorithm for dimension reduction. Mathematically, $$\\mathbf z_n = \\mathbf B^\\top \\mathbf x_n \\in \\mathbb{R}^M$$where $$\\mathbf B := [\\mathbf b_1, \\ldots, \\mathbf b_M] \\in \\mathbb{R}^{D \\times M}\n",
    "$$is the projection matrix. We assume that the columns of $B$ are orthonormal so that $\\mathbf{b}_i^\\top \\mathbf{b}_j = 0$ if and only if $i \\neq j$ and $\\mathbf{b}_i^\\top \\mathbf{b}_i = 1.$ In other words, $\\mathbf b_1, \\ldots, \\mathbf b_M$ form a basis of the $M$-dimensional space, and $z_{in}$ is the weighted sum of $\\mathbf b_i$ (each $\\mathbf b$ is of length $D$) and $\\mathbf x_n$. \n",
    "# Motivation \n",
    "Given the linearity framework, PCA strives to achieve two goals: \n",
    "1. Maximize the variance (the spread of the data) of the low-dimensional representation/projection of the original data (the **without losing information** aspect: Hotelling, 1933): ![](pca_mv.png)For example, PCA will likely to use $x_1$ as the principal component and discard $x_2$ because $x_1$ (or some small change to it) maximizes the variance. \n",
    "2. Minimize the reconstruction error in terms of the squared distance between $\\mathbf {x}$ and  $\\mathbf {\\tilde x}$ (the **accuracy** aspect: Pearson, 1901): $$\\frac{1}{N} \\sum_{n=1}^N \\|\\mathbf x_n - \\tilde{\\mathbf x}_n\\|^2$$\n",
    "**It turns out that there is a *unique* solution that is the key to satisfy both aspects.**\n",
    "\n",
    "# Maximizing Variance\n",
    "The objective is to maximize the variance of the projection $\\mathbf z_n := [z_{1n}, \\ldots, z_{Mn}]$, which are coordinates/codes of a set of $M$ orthonormal basis. In other words, the problem is to choose a subspace that maximize the total variance. \n",
    "\n",
    "Since the basis are orthogonal, the total variance of the $\\mathbf x_n$'s projections to a $M$-dimensional space is the sum of projection variance to each base vector. $$V_{total} = \\sum_{i=1}^M {V_i}$$\n",
    "Therefore, we can break down the problem by finding the $M$-largest orthonormal vectors as the basis of the subspace. \n",
    "\n",
    "We start by seeking vector $\\mathbf b_1$ that maximizes the variance of the projection of $\\mathbf x_n$ to $\\mathbf b_1$, which is the first coordinate of $\\mathbf x_n$'s projection $\\mathbf z_n$: \n",
    "$$z_{1n} = \\mathbf b_1^T \\mathbf x_n \\tag {1}$$\n",
    "Recall that the projection of one vector on to another is $proj_{\\mathbf b_1}{\\mathbf x_n} = \\frac{\\mathbf{x_n} \\cdot \\mathbf{b_1}}{\\|{\\mathbf{b_1}}\\|^2} \\mathbf{b_1}$. We assume $\\mathbf b_1$ is a unit vector, i.e. $\\|{\\mathbf{b_1}}\\|^2 = 1$, and $z_{1n}$ is the coordinate in the direction of $\\mathbf b_1$, we can omit $\\mathbf b_1$ in the end. \n",
    "\n",
    "By exploiting the i.i.d assumption of data ($Var(\\mathbf x_i + \\mathbf x_j) = Var(\\mathbf x_i) + Var(\\mathbf x_j)$), and without loss of generality, by assuming $\\mathbb{E}_\\mathbf x[\\mathbf x] = 0$ (see [Step 1 Center the data](#Step%201%20Center%20the%20data)), we obtain:\n",
    "$$V_1 := V[z_1] = \\frac{1}{N} \\sum_{n=1}^N z_{1n}^2$$\n",
    "By substitution using (1): \n",
    "$$\\begin{align}\n",
    "V_1 &= \\frac{1}{N} \\sum_{n=1}^N (\\mathbf {b}_1^ \\top \\mathbf x_n)^2 = \\frac{1}{N} \\sum_{n=1}^N \\mathbf {b}_1^\\top \\mathbf x_n \\mathbf {x}_n^\\top \\mathbf b_1  \\\\\n",
    "&= \\mathbf {b}_1^\\top \\left( \\frac{1}{N} \\sum_{n=1}^N \\mathbf x_n \\mathbf {x}_n^\\top\\right) \\mathbf b_1 = \\mathbf{b}_1^\\top \\mathbf S \\mathbf b_1\n",
    "\\end{align}\n",
    "$$\n",
    "Where $S$ is the covariance matrix of $\\mathbf X$ (again, we here assume $\\mathbb{E}_\\mathbf x[\\mathbf x] = 0$).\n",
    "To maximize $V_1$, $\\mathbf b_1$ is the eigenvector associated with largest eigenvalue $\\lambda_1$ of $\\mathbf S$ ($\\mathbf S\\mathbf b_1 = \\lambda_1\\mathbf b_1$), and $V_1 = \\lambda_1$. Since we assume $\\mathbf b_1$ to be a unit vector, $\\mathbf b_1$ becomes the first orthonormal base of the subspace $U$ inside $\\mathbb R^D$. Using this method, we find the first principal component.\n",
    "\n",
    "We then subtract out the variance that already explained by the $\\mathbf b_1$ direction and this gets us a new covariance matrix $\\mathbf {\\hat S}$, which has the same eigenvectors and eigenvalues as $\\mathbf S$, except that the eigenvalue associated with $\\mathbf b_1$ is 0. So again, we select the eigenvector $\\mathbf b_2$ that associates with the largest eigenvalue $\\lambda_2$. By induction, the first $M$ eigenvectors of $\\mathbf S$ (ranked by their associating eigenvalues) are the orthonormal vectors that form the basis of the subspace $U$ we want that maximizes the total projection variance.\n",
    "\n",
    "Therefore, we can find all principal components in parallel by first calculating the covariance matrix $\\mathbf S$, and applying eigen-decomposition and extracting the first $M$ eigenvalues and eigenvectors. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a561cfd-5109-4fbb-bdd0-20dd142575aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step-by-step\n",
    "## Step 0: Get data \n",
    "from sklearn import datasets\n",
    "# iris is a dataset that quantifies the morphologic variation of Iris flowers of three related species\n",
    "iris = datasets.load_iris()\n",
    "print(\"Features of the iris dataset: \")\n",
    "print(iris['feature_names'])\n",
    "X = iris['data']\n",
    "print(\"Iris Categories: \")\n",
    "print(iris['target_names'])\n",
    "Y = iris['target']\n",
    "X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f44412-157a-4ecf-b02f-f5c77aa86056",
   "metadata": {},
   "source": [
    "## Step 1: Data Preprocessing \n",
    "### Center the data?\n",
    "In principle, we don't need to center the data, as the variance we are maximizing does not depend on $\\mathbf {\\bar X}: \\operatorname{Var}(\\mathbf z) = \\operatorname{Var}(\\mathbf{B}^{\\top}(\\mathbf{x} - \\boldsymbol{\\mu})) = \\operatorname{Var}(\\mathbf{B}^{\\top}\\mathbf{x} - \\mathbf{B}^{\\top}\\boldsymbol{\\mu}) = \\operatorname{Var}(\\mathbf{B}^{\\top}\\mathbf{x})$. \n",
    "\n",
    "We will also verify this in the example shown later, where we compare the result of applying PCA to both non-centered $\\mathbf X$ and centered $\\mathbf X ^ \\prime$. But in a lot of tutorials they ask you to center the data by subtracting $\\bar x_i, i=1, \\dots, D$, $D$ is the number of dimensions. Why? Because people often either use $\\mathbf X^T\\mathbf X/(n−1)$ to calculate covariance matrix (by omitting the sample mean which effectively assuming mean=0) or they use singular value decomposition to calculate PCs directly, which assumes the data is centered. Either way, it is always safer to center the data. Often in implementation there is library function you can call to center and standardize the data at the same time.\n",
    "### Standardize the data?\n",
    "Standardization divides the data by standard deviation $\\sigma_i, i=1, \\dots, D$ so that each dimension is unit free and has variance 1. This step is necessary (unlike the centering part), because recall that the PCA's goal is to maximize the variance. If a certain dimension in $\\mathbf X$ is disproportionally large in magnitude, the variance PCA in turn try to maximize will  disproportionally be biased towards this dimension. For example, if dimension $i$ is measuring length in meter, its variance will be 10000 more by changing its unit to centimeters. In short, unit in data games the PCA system. Standardization prevents this. \n",
    "\n",
    "A nice feature that comes with standardization is it turns the covariance matrix $\\mathbf S$ into correlation matrix $\\mathbf C$: recall that $$Corr = \\frac{\\text{Cov}(X_i, X_j)}{\\sqrt{\\text{Var}(X_i) \\text{Var}(X_j)}} = \\text{Cov}(X_i, X_j)​.$$Since each $\\mathbf x$ is unit variance. \n",
    "\n",
    "And we know that principal components are orthogonal, i.e., they are not correlated, we can interpret PCA as effectively *de-correlating* the original $D$ dimensions of the data matrix $\\mathbf X$, which circles back to what we've discussed in [Dimension Reduction](#Dimension%20Reduction).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2615540-6bec-4566-bd1a-b9ecc773c6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Data Preprocessing \n",
    "# Before preprocessing \n",
    "print(f\"Before preprocessing: Average length of sepals: {X[:,0].mean()}\")\n",
    "print(f\"Before preprocessing: Average length of sepals: {X[:,0].var()}\")\n",
    "# center and standardize the data \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "std_X = StandardScaler().fit_transform(X)\n",
    "# After standarization\n",
    "print(f\"After preprocessing: Average length of sepals: {std_X[:,0].mean()}\")\n",
    "print(f\"After preprocessing: Average length of sepals: {std_X[:,0].var()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a8502d-c10c-4c3c-a659-8ab77334eadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Calculate the Covariance Matrix\n",
    "import numpy as np\n",
    "cov_matrix = np.cov(std_X, rowvar=False)\n",
    "# or manually: we can use this formula to calculate covariance because we already centered the data\n",
    "cov_matrix_manual = (std_X.T @ std_X) / (std_X.shape[0] - 1)\n",
    "np.isclose(cov_matrix, cov_matrix_manual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528bf8cc-34ed-4ebd-885f-f9b2d919ec15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Eigen-Decomposition\n",
    "from scipy.linalg import eigh\n",
    "values, vectors = eigh(cov_matrix)\n",
    "print(f\"eigenvalues in ascending order (aka, the projection variance): {values}\")\n",
    "# it seems like the first two pc already did pretty well \n",
    "pcs = vectors[:,2:]\n",
    "# turn the ascending order into descending\n",
    "pcs = np.flipud(pcs.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf0b0f1-3629-41dc-93b4-3533b5f95d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 4: Mapping data to subspace U\n",
    "new_coordinates_man = (pcs @ std_X.T).T\n",
    "plt.scatter(new_coordinates_man[:, 0], new_coordinates_man[:, 1], c=Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a93df20-c739-4b0b-b5a1-6fd14e66b690",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using library code\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "new_coordinates = pca.fit_transform(std_X) # fit: our step 3; transform: our step 4\n",
    "eigenvalues = pca.explained_variance_\n",
    "eigenvectors = pca.components_\n",
    "eigenvectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22465901-03f6-4ab8-9e6e-4a5e536ca3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254d678f-a8a6-425a-822e-c5c103a6f5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(new_coordinates[:, 0], new_coordinates[:, 1], c=Y)\n",
    "plt.xlabel(f'Principal Component 1: {round(pca.explained_variance_ratio_[0]*100, 2)}%')\n",
    "plt.ylabel(f'Principal Component 2: {round(pca.explained_variance_ratio_[1]*100, 2)}%')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22eae3ae-384f-4314-a8c0-9d294e61799c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot in 3D\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54503ca-791d-4241-8d0d-6b8ee47d134c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA analysis \n",
    "## Plotting Contribution of Original Variables to Principal Components\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "loadings = pca.components_.T\n",
    "loadings_df = pd.DataFrame(loadings, columns=['PC1', 'PC2'], index=iris['feature_names'])\n",
    "sns.heatmap(loadings_df, annot=True, cmap='coolwarm', center=0)\n",
    "plt.title('Contribution of Original Variables to Principal Components')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed79fee-8163-4e0a-a8f0-bacac5f21686",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot cumulative explained variance in terms of number of PCs\n",
    "# let number of PCs = number of features: so that we can have the cumulative distribution of explained variances\n",
    "pca2 = PCA(n_components=std_X.shape[1])\n",
    "pca2.fit(std_X)\n",
    "cumulative_explained_variance = np.cumsum(pca2.explained_variance_ratio_)\n",
    "x_values = np.arange(1, len(cumulative_explained_variance) + 1)\n",
    "plt.plot(x_values, cumulative_explained_variance, marker='o', linestyle='-', color='b')\n",
    "plt.title('Cumulative Explained Variance')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.xticks(x_values)\n",
    "# Optionally, add a horizontal line to mark a specific threshold, e.g., 95% variance explained\n",
    "threshold = 0.95\n",
    "plt.axhline(y=threshold, color='r', linestyle='--')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2ff454-7b8f-4810-9782-9f2fb1f0b789",
   "metadata": {},
   "source": [
    "# Projecting back\n",
    "We may also want to project the $\\mathbf z_n$ back to $\\mathbf {\\tilde x_n} = \\mathbf B \\mathbf z_n=\\mathbf B \\mathbf B ^\\top \\mathbf x_n \\in \\mathbb R^D$ , which live in a lower dimensional subspace $U \\subseteq \\mathbb R^D, dim(U) = M$. For example, if $D = 3, M = 2$, then $\\mathbf {\\tilde x_n}$ and $\\mathbf z_n$ are both projections of $\\mathbf x_n$s: $\\mathbf {\\tilde x_n}$ are 3D vectors that lie in the 2D plane $U$, whereas $\\mathbf z_n$ are 2D vectors. They are both projections, but one is viewed in the coordinate system of $\\mathbb R^D$, the other one is viewed in terms of the subspace $U$, specifically its orthonormal basis $\\mathbf b_1, \\ldots, \\mathbf b_M$. \n",
    "\n",
    "The reason we are using $\\mathbf B$ to project back is because it minimizes the euclidean distance between $\\mathbf {x_n}$ and $\\mathbf {\\tilde x_n}$, aka, the reconstruction error. \n",
    "\n",
    "We can think of PCA as a linear encoder $\\mathbf B^\\top$ that maximizes the variance and a linear decoder that minimizes the error.\n",
    "\n",
    "# Limitations of PCA\n",
    "- **Assumption of Linearity**. Recent success in Machine learning, specifically in deep learning, in some sense is solely depends on its ability to learn to model extremely complex non-linear functions. For example, if the original data points are in $\\mathbb R^3$ , and we choose $M=2$, PCA will get us the optimal 2D plane that maximizes the projection variance and minimizes the reconstruction error. All good. But there must exists a \"curly plane\" or some much more complex structure that can account for more variance and has lower error. \n",
    "- **Variance as Information**: Variance is the bread and butter to PCA. What if the variance is \"contaminated\" by noise? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c884396b-dae9-4bdd-b2e4-dcd63bba7025",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
